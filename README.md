############ AIer##########################
This repository are used to log AI knowledge

###########EWMA(Exponentially Weighted Moving Average)指数加权移动平均##############
EWMA(t) = aY(t) + (1-a)EWMA(t-1)
EWMA(t)表示t时刻的估计值，Y(t)为观测值，a(0<a<1)为历史观测值的权重系数，决定估计器跟踪实际数据突变的能力，即时效性。a越大时效性越强，平稳性下降（风机选取为0.2）。EWMA相当于一个低通滤波器，控制输入值，剔除短期波动，保留长期发展趋势。
第100个数据其实是前99个数据加权和，而前面每一个数的权重呈现指数衰减，即越靠前的数据对当前结果的影响较小。

############优化算法################################################################
#批量梯度下降（Batch gradient descent，BGD）
每迭代一步，都要用到训练集的所有数据，每次计算出来的梯度求平均
优点：cost fuction若为凸函数，能够保证收敛到全局最优值；若为非凸函数，能够收敛到局部最优值
缺点：
1.由于每轮迭代都需要在整个数据集上计算一次，所以批量梯度下降可能非常慢
2.训练数较多时，需要较大内存
3.批量梯度下降不允许在线更新模型，例如新增实例。

#随机梯度下降（Stochastic Gradient Descent，SGD）
通过每个样本来迭代更新一次，以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。
优点:
1.算法收敛速度快(在Batch Gradient Descent算法中, 每轮会计算很多相似样本的梯度, 这部分是冗余的)
2.可以在线更新
3.有几率跳出一个比较差的局部最优而收敛到一个更好的局部最优甚至是全局最优
缺点：
1.对于参数比较敏感，需要注意参数的初始化 
2.容易陷入局部极小值，且容易困在鞍点，由于频繁的更新和波动，最终将收敛到最小限度，并会因波动频繁存在超调量。
3.当数据较多时，训练时间长 
4.每迭代一步，都要用到训练集所有的数据

#小批量梯度下降（Mini Batch Gradient Descent，MBGD）
为了避免SGD和标准梯度下降中存在的问题，对每个批次中的n个训练样本，这种方法只执行一次更新。【每次更新全部梯度的平均值】，通常来说，小批量样本的大小范围是从50到256，可以根据实际问题而有所不同。
具体实现: 
需要:学习速率 , 初始参数  
每步迭代过程: 
1. 从训练集中的随机抽取一批容量为m的样本,以及相关的输出 
2. 计算梯度和误差并更新参数: 
优点: 
训练速度快,对于很大的数据集,也能够以较快的速度收敛.

缺点: 
1. 由于是抽取,因此不可避免的,得到的梯度肯定有误差.因此学习速率需要逐渐减小.否则模型无法收敛 
因为误差,所以每一次迭代的梯度受抽样的影响比较大,也就是说梯度含有比较大的噪声,不能很好的反映真实梯度.
2. 选择合适的learning rate比较困难- 对所有的参数更新使用同样的learning rate。对于稀疏数据或者特征，有时我们可能想更新快一些对于不经常出现的特征，对于常出现的特征更新慢一些，这时候SGD就不太能满足要求了
3. SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点

#Momentum（动量梯度下降法）
momentum是模拟物理里动量的概念，积累之前的动量来替代真正的梯度。通过优化相关方向的训练和弱化无关方向的振荡，来加速SGD训练，Momentum方法也可以帮助跳出局部极值。上面的MBGD有个问题,就是每次迭代计算的梯度含有比较大的噪音。Momentum在每次下降时都加上之前运动方向上的动量,在梯度缓慢的维度下降更快，在梯度险峻的维度减少抖动。对于在梯度点处具有相同的方向的维度，其动量项增大，对于在梯度点处改变方向的维度，其动量项减小。因此，我们可以得到更快的收敛速度，同时可以减少摇摆。动量项γ通常设定为0.9

#Nesterov
nesterov项在梯度更新时做一个校正，避免前进太快，同时提高灵敏度。比单纯的Momentum效果好。高动量可能会导致其完全地错过最小值，应该提前减速。
他提出先根据之前的动量进行大步跳跃，然后计算梯度进行校正，从而实现参数更新。这种预更新方法能防止大幅振荡，不会错过最小值，并对参数更新更加敏感。

#Adagrad
adagrad方法是将每一个参数的每一次迭代的梯度取平方累加再开方，用基础学习率除以这个数，来做学习率的动态更新。【这样每一个参数的学习率就与他们的梯度有关系了，那么每一个参数的学习率就不一样了！也就是所谓的自适应学习率】 
Adagrad的一大优势时可以避免手动调节学习率，比如设置初始的缺省学习率为0.01，然后就不管它，另其在学习的过程中自己变化。
缺点：
计算时要在分母上计算梯度平方的和，由于所有的参数平方必为正数，这样就造成在训练的过程中，分母累积的和会越来越大。这样学习到后来的阶段，网络的更新能力会越来越弱，能学到的更多知识的能力也越来越弱，因为学习率会变得极其小【就会提前停止学习】，为了解决这样的问题又提出了Adadelta算法。

#Adadelta
Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项【其实就是相当于指数滑动平均，只用了前多少步的梯度平方平均值】
如果这次梯度大,那么学习速率衰减的就快一些;如果这次梯度小,那么学习速率衰减的就慢一些。
让学习率适应参数，对于出现次数较少的特征，我们对其采用更大的学习率，对于出现次数较多的特征，我们对其采用较小的学习率。我们甚至都无需设置默认的学习率，因为更新规则中已经移除了学习率。

#RMSprop
RMSprop可以算作Adadelta的一个特例,求梯度平方和的平均数，再求根就变成了RMS(均方根)。其实RMSprop依然依赖于全局学习率，对于RNN效果很好。
Adagrad会累加之前所有的梯度平方，而RMSprop仅仅是计算对应的平均值，因此可缓解Adagrad算法学习率下降较快的问题。 

#Adam
之前的方法中计算了每个参数的对应学习率，但是为什么不计算每个参数的对应动量变化并独立存储。结合了Momentum和RMSprop的特点,自适应学习率，Adam(Adaptive Moment Estimation)本质上是带有动量项的RMSprop，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率，Adam集成了SGD的一阶动量和RMSProp的二阶动量。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。
特点：
结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点
对内存需求较小
为不同的参数计算不同的自适应学习率
也适用于大多非凸优化- 适用于大数据集和高维空间

#Adamax
Adamax是Adam的一种变体，此方法对学习率的上限提供了一个更简单的范围
#Nadam
Nadam类似于带有Nesterov动量项的Adam。

#总结，经验之谈
对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值
SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠
如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。
Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。
在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果

优化器的目的：1. 加速收敛 2. 防止过拟合 3. 防止局部最优
