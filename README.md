############ AIer##########################
This repository are used to log AI knowledge

###########EWMA(Exponentially Weighted Moving Average)指数加权移动平均##############
EWMA(t) = aY(t) + (1-a)EWMA(t-1)
EWMA(t)表示t时刻的估计值，Y(t)为观测值，a(0<a<1)为历史观测值的权重系数，决定估计器跟踪实际数据突变的能力，即时效性。a越大时效性越强，平稳性下降（风机选取为0.2）。EWMA相当于一个低通滤波器，控制输入值，剔除短期波动，保留长期发展趋势。
第100个数据其实是前99个数据加权和，而前面每一个数的权重呈现指数衰减，即越靠前的数据对当前结果的影响较小。

############优化算法################################################################
#批量梯度下降（Batch gradient descent，BGD）
每迭代一步，都要用到训练集的所有数据，每次计算出来的梯度求平均

#随机梯度下降（Stochastic Gradient Descent，SGD）
通过每个样本来迭代更新一次，以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。
缺点：
对于参数比较敏感，需要注意参数的初始化 
容易陷入局部极小值 
当数据较多时，训练时间长 
每迭代一步，都要用到训练集所有的数据

#小批量梯度下降（Mini Batch Gradient Descent，MBGD）
为了避免SGD和标准梯度下降中存在的问题，对每个批次中的n个训练样本，这种方法只执行一次更新。【每次更新全部梯度的平均值】

#Momentum（动量梯度下降法）
momentum是模拟物理里动量的概念，积累之前的动量来替代真正的梯度。Momentum方法也可以帮助跳出局部极值。

#Adagrad
adagrad方法是将每一个参数的每一次迭代的梯度取平方累加再开方，用基础学习率除以这个数，来做学习率的动态更新。【这样每一个参数的学习率就与他们的梯度有关系了，那么每一个参数的学习率就不一样了！也就是所谓的自适应学习率】 
Adagrad的一大优势时可以避免手动调节学习率，比如设置初始的缺省学习率为0.01，然后就不管它，另其在学习的过程中自己变化。当然它也有缺点，就是它计算时要在分母上计算梯度平方的和，由于所有的参数平方必为正数，这样就造成在训练的过程中，分母累积的和会越来越大。这样学习到后来的阶段，网络的更新能力会越来越弱，能学到的更多知识的能力也越来越弱，因为学习率会变得极其小【就会提前停止学习】，为了解决这样的问题又提出了Adadelta算法。
Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项【其实就是相当于指数滑动平均，只用了前多少步的梯度平方平均值】
