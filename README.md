############ AIer##########################
This repository are used to log AI knowledge

###########EWMA(Exponentially Weighted Moving Average)指数加权移动平均##############
EWMA(t) = aY(t) + (1-a)EWMA(t-1)
EWMA(t)表示t时刻的估计值，Y(t)为观测值，a(0<a<1)为历史观测值的权重系数，决定估计器跟踪实际数据突变的能力，即时效性。a越大时效性越强，平稳性下降（风机选取为0.2）。EWMA相当于一个低通滤波器，控制输入值，剔除短期波动，保留长期发展趋势。
第100个数据其实是前99个数据加权和，而前面每一个数的权重呈现指数衰减，即越靠前的数据对当前结果的影响较小。

############优化算法################################################################
#批量梯度下降（Batch gradient descent，BGD）
每迭代一步，都要用到训练集的所有数据，每次计算出来的梯度求平均

#随机梯度下降（Stochastic Gradient Descent，SGD）
通过每个样本来迭代更新一次，以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。
缺点：
对于参数比较敏感，需要注意参数的初始化 
容易陷入局部极小值 
当数据较多时，训练时间长 
每迭代一步，都要用到训练集所有的数据

#小批量梯度下降（Mini Batch Gradient Descent，MBGD）
为了避免SGD和标准梯度下降中存在的问题，对每个批次中的n个训练样本，这种方法只执行一次更新。【每次更新全部梯度的平均值】
